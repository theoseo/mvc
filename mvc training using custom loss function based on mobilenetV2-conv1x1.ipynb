{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changed Model archietecter to two conv1x1 cnn\n",
    "\n",
    "Test 1\n",
    "conv1x1(512)->conv1x1(27)->global_avg_pool ->flatten(27)\n",
    "image flop only horizontoal \n",
    "Test 78%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_pickle('./mvc_train.pkl')\n",
    "test_df = pd.read_pickle('./mvc_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>category/Pants/p8652936_s3446722_v2.jpg</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61400</th>\n",
       "      <td>category/Coats_Outerwear/p8620072_s3365471_v2.jpg</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62479</th>\n",
       "      <td>category/Coats_Outerwear/p8326527_s3170110_v2.jpg</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107817</th>\n",
       "      <td>category/Dresses/p8623092_s3372473_v3.jpg</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48750</th>\n",
       "      <td>category/Shirts_Tops/p8569580_s3259101_v3.jpg</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61018</th>\n",
       "      <td>category/Shirts_Tops/p8607347_s3338577_v3.jpg</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56496</th>\n",
       "      <td>category/Pants/p8523650_s3146327_v3.jpg</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81567</th>\n",
       "      <td>category/Shirts_Tops/p8547705_s3308942_v1.jpg</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61977</th>\n",
       "      <td>category/Jeans/p8655658_s3452882_v3.jpg</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79893</th>\n",
       "      <td>category/Pants/p8282942_s2590117_v3.jpg</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 filename  \\\n",
       "1779              category/Pants/p8652936_s3446722_v2.jpg   \n",
       "61400   category/Coats_Outerwear/p8620072_s3365471_v2.jpg   \n",
       "62479   category/Coats_Outerwear/p8326527_s3170110_v2.jpg   \n",
       "107817          category/Dresses/p8623092_s3372473_v3.jpg   \n",
       "48750       category/Shirts_Tops/p8569580_s3259101_v3.jpg   \n",
       "61018       category/Shirts_Tops/p8607347_s3338577_v3.jpg   \n",
       "56496             category/Pants/p8523650_s3146327_v3.jpg   \n",
       "81567       category/Shirts_Tops/p8547705_s3308942_v1.jpg   \n",
       "61977             category/Jeans/p8655658_s3452882_v3.jpg   \n",
       "79893             category/Pants/p8282942_s2590117_v3.jpg   \n",
       "\n",
       "                                               attributes  \n",
       "1779    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "61400   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "62479   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "107817  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "48750   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "61018   [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "56496   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "81567   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "61977   [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "79893   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['BoardShorts',\n",
    "'ButtonUpShirts',\n",
    "'Crew',\n",
    "'Denim',\n",
    "'FloralPrint',\n",
    "'GraphicPrint',\n",
    "'Halter',\n",
    "'Hood',\n",
    "'HorizontalStripes',\n",
    "'Insulated',\n",
    "'Leggings',\n",
    "'LongSleeves',\n",
    "'Mesh',\n",
    "'Plaid',\n",
    "'Polos',\n",
    "'Scoop',\n",
    "'ShortSleeves',\n",
    "'Sleeveless',\n",
    "'SnowPants',\n",
    "'Stripes',\n",
    "'VerticalStripes',\n",
    "'Vests',\n",
    "'Vneck',\n",
    "'Zipper',\n",
    "'halfZip',\n",
    "'quarter3Zip',\n",
    "'quarterZip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "CROP_SIZE = 224\n",
    "\n",
    "def train_preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = (image/127.5) - 1\n",
    "    image = tf.image.random_crop(image, [CROP_SIZE, CROP_SIZE, 3])\n",
    "    #image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    #image /= 255.0  # normalize to [0,1] range\n",
    "    \n",
    "\n",
    "    #encoded_png_io = io.BytesIO(encoded_png)\n",
    "    #image = pil.open(encoded_png_io)\n",
    "    #image = np.asarray(image)    \n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [CROP_SIZE, CROP_SIZE])\n",
    "    #image = tf.image.random_crop(image, [CROP_SIZE, CROP_SIZE, 3])\n",
    "    image = (image/127.5) - 1\n",
    "    #image /= 255.0  # normalize to [0,1] range\n",
    "    \n",
    "\n",
    "    #encoded_png_io = io.BytesIO(encoded_png)\n",
    "    #image = pil.open(encoded_png_io)\n",
    "    #image = np.asarray(image)    \n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_load_and_preprocess_image(path):\n",
    "    image = tf.read_file(path)\n",
    "    return train_preprocess_image(image)\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.read_file(path)\n",
    "    return preprocess_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_paths = train_df['filename'].tolist()\n",
    "train_label = np.asarray(train_df['attributes'].tolist())\n",
    "\n",
    "test_paths = test_df['filename'].tolist()\n",
    "test_label = np.asarray(test_df['attributes'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_ds = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "test_path_ds = tf.data.Dataset.from_tensor_slices(test_paths)\n",
    "\n",
    "train_image_ds = train_path_ds.map(train_load_and_preprocess_image)\n",
    "test_image_ds = test_path_ds.map(load_and_preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(train_label, tf.int32))\n",
    "test_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(test_label, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((224, 224, 3), (27,)), types: (tf.float32, tf.int32)>\n",
      "<DatasetV1Adapter shapes: ((224, 224, 3), (27,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "train_image_label_ds = tf.data.Dataset.zip((train_image_ds, train_label_ds))\n",
    "print(train_image_label_ds)\n",
    "test_image_label_ds = tf.data.Dataset.zip((test_image_ds, test_label_ds))\n",
    "print(test_image_label_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 224, 224, 3), (?, 27)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
    "# completely shuffled.\n",
    "\n",
    "train_image_label_ds = train_image_label_ds.shuffle(buffer_size=512).repeat()\n",
    "train_image_label_ds = train_image_label_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "train_image_label_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 224, 224, 3), (?, 27)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_label_ds = test_image_label_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "test_image_label_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape:  (32, 224, 224, 3)\n",
      "Label batch shape:  (32, 27)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in train_image_label_ds:\n",
    "    print(\"Image batch shape: \", image_batch.shape)\n",
    "    print(\"Label batch shape: \", label_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVCMobilenetV2(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, img_shape, num_attributes):\n",
    "        super(MVCMobilenetV2, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "        self.num_attributes = num_attributes\n",
    "        \n",
    "        self.base_model = tf.keras.applications.MobileNetV2(input_shape=self.img_shape, include_top=False, weights='imagenet')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (CROP_SIZE, CROP_SIZE, 3)\n",
    "\n",
    "\n",
    "#base_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(input_shape=IMG_SHAPE,\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "#base_model = tf.keras.applications.nasnet.NASNetMobile(input_shape=IMG_SHAPE,\n",
    "#base_model = tf.keras.applications.densenet.DenseNet201(input_shape=IMG_SHAPE,\n",
    "#base_model = tf.keras.applications.nasnet.NASNetLarge(input_shape=IMG_SHAPE,\n",
    "#base_model = tf.keras.applications.resnet50.ResNet50(input_shape=IMG_SHAPE,\n",
    "#base_model = tf.keras.applications.xception.Xception(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.output_shape\n",
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv 1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_layer = tf.keras.layers.Conv2D(512, kernel_size=1)\n",
    "conv2_layer = tf.keras.layers.Conv2D(27, kernel_size=1)\n",
    "avgpool_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "flatten_layer = tf.keras.layers.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    conv1_layer,\n",
    "    conv2_layer,    \n",
    "    avgpool_layer,\n",
    "    flatten_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'flatten/Identity:0' shape=(?, 27) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 7, 7, 512)         655872    \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 27)          13851     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 27)                0         \n",
      "=================================================================\n",
      "Total params: 2,927,707\n",
      "Trainable params: 2,893,595\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27348"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_train_samples = len(train_paths) \n",
    "nb_val_samples = len(test_paths) \n",
    "\n",
    "steps_per_epoch = nb_train_samples // BATCH_SIZE\n",
    "validation_steps=nb_val_samples//BATCH_SIZE\n",
    "nb_val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(target, prediction):\n",
    "    batch = tf.shape(target)[0]\n",
    "    \n",
    "    #print(batch)\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    weight = 1-tf.divide(tf.reduce_sum(target,0, keepdims=True),tf.cast(batch, tf.float32))\n",
    "    #print(weight)\n",
    "    #print(loss)\n",
    "    return tf.nn.weighted_cross_entropy_with_logits(labels=target, logits=prediction, pos_weight=weight)\n",
    "    \n",
    "    #return tf.nn.weighted_cross_entropy_with_logits(targets=tf.cast(y, tf.float32), logits=y_, pos_weight=tf.cast(weight, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when val is very slow refer this [page](https://github.com/keras-team/keras/issues/6101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallel_model = multi_gpu_model(model, gpus=2)\n",
    "callbacks = [\n",
    "    # Interrupt training if `val_loss` stops improving for over 2 epochs\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='./mobilev2_conv2/full_aug.top.h5', save_best_only=True),\n",
    "    # Write TensorBoard logs to `./logs` directory\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./mobilev2_conv2/logs')\n",
    "]\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss=weighted_loss, optimizer='adam', metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "#model.load_weights('./mobilev2_2019072004/weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3418/3418 [==============================] - 2070s 606ms/step - loss: 0.0751 - binary_accuracy: 0.9698 - val_loss: 0.1801 - val_binary_accuracy: 0.9426\n",
      "Epoch 2/20\n",
      "3418/3418 [==============================] - 2017s 590ms/step - loss: 0.0622 - binary_accuracy: 0.9750 - val_loss: 0.0806 - val_binary_accuracy: 0.9691\n",
      "Epoch 3/20\n",
      "3418/3418 [==============================] - 2028s 593ms/step - loss: 0.0577 - binary_accuracy: 0.9767 - val_loss: 0.0717 - val_binary_accuracy: 0.9708\n",
      "Epoch 4/20\n",
      "3418/3418 [==============================] - 2020s 591ms/step - loss: 0.0545 - binary_accuracy: 0.9779 - val_loss: 0.0629 - val_binary_accuracy: 0.9750\n",
      "Epoch 5/20\n",
      "3418/3418 [==============================] - 2020s 591ms/step - loss: 0.0520 - binary_accuracy: 0.9788 - val_loss: 0.0652 - val_binary_accuracy: 0.9723\n",
      "Epoch 6/20\n",
      "3418/3418 [==============================] - 2029s 594ms/step - loss: 0.0499 - binary_accuracy: 0.9796 - val_loss: 0.0722 - val_binary_accuracy: 0.9726\n",
      "Epoch 7/20\n",
      "3418/3418 [==============================] - 2020s 591ms/step - loss: 0.0481 - binary_accuracy: 0.9802 - val_loss: 0.0549 - val_binary_accuracy: 0.9776\n",
      "Epoch 8/20\n",
      "3418/3418 [==============================] - 2018s 590ms/step - loss: 0.0464 - binary_accuracy: 0.9809 - val_loss: 0.0616 - val_binary_accuracy: 0.9740\n",
      "Epoch 9/20\n",
      "3418/3418 [==============================] - 2028s 593ms/step - loss: 0.0448 - binary_accuracy: 0.9814 - val_loss: 0.0589 - val_binary_accuracy: 0.9761\n",
      "Epoch 10/20\n",
      "3418/3418 [==============================] - 2019s 591ms/step - loss: 0.0434 - binary_accuracy: 0.9820 - val_loss: 0.0600 - val_binary_accuracy: 0.9759\n"
     ]
    }
   ],
   "source": [
    "#with tf.device('/cpu:0'): \n",
    "history = model.fit(train_image_label_ds, steps_per_epoch=steps_per_epoch, epochs=20, validation_data=test_image_label_ds, validation_steps=validation_steps, \\\n",
    "          workers=4, use_multiprocessing=True , callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./mobilev2_conv2/full_aug.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./mobilev2_conv2/full_aug.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17    Sleeveless\n",
       "22         Vneck\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='./rename/p8604244_s3328909_v0.jpg'\n",
    "image = tf.read_file(path)\n",
    "image = tf.image.decode_jpeg(image, channels=3)\n",
    "image = tf.image.resize(image, [224,224])\n",
    "image = image/127.5 - 1 # normalize to [0,1] range\n",
    "pred = model.predict(image[np.newaxis, ...])\n",
    "indexes = np.where(pred[0,:]> 0.5)\n",
    "attr=pd.Series(label)\n",
    "attr.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "acc = history.history['binary_accuracy']\n",
    "val_acc = history.history['val_binary_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
